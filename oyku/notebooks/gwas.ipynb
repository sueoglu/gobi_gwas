{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4718c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas-plink limix-lmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas_plink import read_plink\n",
    "from limix_lmm import LMM\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd7a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.stats as st\n",
    "\n",
    "if not hasattr(scipy, 'dot'):\n",
    "    scipy.dot = np.dot\n",
    "if not hasattr(scipy, 'einsum'):\n",
    "    scipy.einsum = np.einsum\n",
    "if not hasattr(scipy, 'log'):\n",
    "    scipy.log = np.log\n",
    "if not hasattr(scipy, 'sign'):\n",
    "    scipy.sign = np.sign\n",
    "if not hasattr(scipy, 'sqrt'):\n",
    "    scipy.sqrt = np.sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dbe785",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline\n",
    "\n",
    "BFILE=/Users/oykusuoglu/gobi/gobi_gwas/oyku/data/ALL.chr22_GRCh38.genotypes.20170504\n",
    "\n",
    "OUTDIR=/Users/oykusuoglu/gobi/gobi_gwas/oyku/data/qc\n",
    "\n",
    "mkdir -p \"$OUTDIR\"\n",
    "\n",
    "\n",
    "plink --bfile \"$BFILE\" --freq --missing --out \"$OUTDIR\"/initial_qc\n",
    "\n",
    "\n",
    "Preprocessing \n",
    "\n",
    "We only kept SNP like variants (single base substitutions), dropped variants with missing rate > 0.02 and minor allele frequency < 0.01 to keep only commons, and also variants that have extreme deviations that are caused by noise rather than signal\n",
    "\n",
    "(filtered data set written as a new bed/bim/fam)\n",
    "\n",
    "\n",
    "plink --bfile \"$BFILE\" \\\n",
    "  --snps-only \\\n",
    "  --geno 0.02 \\\n",
    "  --maf 0.01 \\\n",
    "  --hwe 1e-6 \\\n",
    "  --make-bed \\\n",
    "  --out \"$OUTDIR\"/chr22_step1_common\n",
    "\n",
    "\n",
    "check if there are duplicate variant ids\n",
    "\n",
    "plink --bfile \"$OUTDIR\"/chr22_step1_common \\\n",
    "  --list-duplicate-vars ids-only suppress-first \\\n",
    "  --out \"$OUTDIR\"/dupcheck\n",
    "\n",
    "wc -l \"$OUTDIR\"/dupcheck.dupvar\n",
    "\n",
    "there were no duplicates (0)\n",
    "\n",
    "plink --bfile \"$OUTDIR\"/chr22_step1_common \\\n",
    "  --exclude \"$OUTDIR\"/dupcheck.dupvar \\\n",
    "  --make-bed \\\n",
    "  --out \"$OUTDIR\"/chr22_step2_nodup\n",
    "\n",
    "sample level missingness, none was filtered (both chr22_step2_nodup.fam and chr22_step3_sampleqc.fam are same size 2504)\n",
    "\n",
    "plink --bfile \"$OUTDIR\"/chr22_step2_nodup \\\n",
    "  --mind 0.02 \\\n",
    "  --make-bed \\\n",
    "  --out \"$OUTDIR\"/chr22_step3_sampleqc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e96f0",
   "metadata": {},
   "source": [
    "INITIAL:\n",
    "\n",
    "Number of variants: 109827 \n",
    "\n",
    "Number of samples: 2504\n",
    "\n",
    "AFTER QC:\n",
    "\n",
    "Number of variants: 59743\n",
    "\n",
    "Number of samples: 2504\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b22dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bfile = '/Users/oykusuoglu/gobi/gobi_gwas/oyku/data/qc/chr22_step3_sampleqc'\n",
    "bfile = '/Users/oykusuoglu/gobi/gobi_gwas/oyku/data/pca/before_qc/qc/chr22_step3_sampleqc'\n",
    "bim, fam, G = read_plink(bfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24507c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_snps = bim.shape[0]\n",
    "n_samples = fam.shape[0]\n",
    "n_snps, n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real = G.compute().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b132609a",
   "metadata": {},
   "source": [
    "Standardize the genotype matrix, so that all SNPs are on the same scale, 3 variants were dropped, which had a standard deviation equals to 0 among all individuals. (bim annotation table must be also updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole chromosome 22\n",
    "\n",
    "mu_full = X_real.mean(axis=0)\n",
    "sd_full = X_real.std(axis=0, ddof=0)\n",
    "keep_full = sd_full > 1e-12\n",
    "keep_idx  = np.where(keep_full)[0] \n",
    "standardized_X = (X_real[:, keep_full] - mu_full[keep_full]) / sd_full[keep_full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e689eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bim data for variants that werent dropped after the standardization\n",
    "bim_kept = bim.iloc[keep_idx].copy().reset_index(drop=True)\n",
    "\n",
    "bim_kept[\"orig_bim_idx\"] = keep_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e195c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ecb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_X[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339425d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6febc841",
   "metadata": {},
   "source": [
    "### Phase 1: Phenotype Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d8d8f5",
   "metadata": {},
   "source": [
    "First, we need to set the parameters (heritability and number of casual SNPs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bbdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_SEED = 42\n",
    "\n",
    "def seed_for(h2, m, base=BASE_SEED):\n",
    "    h2_int = int(round(h2 * 1_000_000))  \n",
    "    return base + 1_000_003 * h2_int + 10_007 * int(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfb5fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "M = 10 \n",
    "h2 = 0.6 \n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "idx_caus = rng.choice(standardized_X.shape[1], size=M, replace=False)\n",
    "var_expl = np.repeat(h2/M, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a08c037",
   "metadata": {},
   "source": [
    "Code from GWAS exercise to simulate phenotypes (maybe delete the standardization part from here since its been done beforehand):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899083f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_pheno(X, idx_caus, var_expl, rng, direction=None):\n",
    "    # Ensure that the number of causal variant indices matches the number of variances explained.\n",
    "    assert len(idx_caus) == len(var_expl)\n",
    "\n",
    "    # If no direction is provided, randomly assign a positive or negative direction for each causal variant.\n",
    "    if direction is None:\n",
    "        direction = 2. * (rng.random(len(idx_caus)) > 0.5) - 1.\n",
    "    # Ensure that the number of directions matches the number of causal variant indices.\n",
    "    assert len(idx_caus) == len(direction)\n",
    "\n",
    "    # Compute the remaining variance after accounting for the variance explained by the causal variants.\n",
    "    ve = 1 - var_expl.sum()\n",
    "    # Ensure that the total variance explained by causal variants is less than 1.\n",
    "    assert ve > 0, 'sum(var_expl) should be < 1'\n",
    "\n",
    "    # Compute the effect sizes for the causal variants based on the variance they explain and their direction.\n",
    "    beta = np.sqrt(var_expl) * direction\n",
    "\n",
    "    # Extract the columns of X corresponding to the causal variants and standardize them.\n",
    "    Xc = X[:, idx_caus]\n",
    "    Xc = (Xc - Xc.mean(0)) / Xc.std(0)\n",
    "\n",
    "    # Compute the genetic component of the phenotype.\n",
    "    yg = Xc.dot(beta)[:, None]\n",
    "    # Compute the noise component of the phenotype.\n",
    "    yn = np.sqrt(ve) * rng.standard_normal((X.shape[0], 1))\n",
    "\n",
    "    # Sum the genetic and noise components to get the simulated phenotype.\n",
    "    y = yg + yn\n",
    "\n",
    "    # Initialize the real effect sizes for all variants in X as zeros.\n",
    "    beta_real = np.zeros(X.shape[1])\n",
    "    # Update the real effect sizes for the causal variants.\n",
    "    beta_real[idx_caus] = beta\n",
    "\n",
    "    # Standardize the phenotypic values to have mean 0 and standard deviation 1.\n",
    "    ystd = y.std()\n",
    "    y = (y - y.mean()) / ystd\n",
    "    # Adjust the real effect sizes accordingly after standardizing y.\n",
    "    beta_real = beta_real / ystd\n",
    "\n",
    "    return y, beta_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qq_plot(p_values, title):\n",
    "    \"\"\"\n",
    "    Create a QQ plot given a list of p-values.\n",
    "\n",
    "    Parameters:\n",
    "    - p_values: list of p-values\n",
    "    - title: title for the plot\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort p-values\n",
    "    observed = -np.log10(np.sort(p_values))\n",
    "    expected = -np.log10(np.arange(1, len(p_values) + 1) / (len(p_values) + 2))\n",
    "\n",
    "    # Create the QQ plot\n",
    "    plt.scatter(expected, observed, marker='.')\n",
    "    plt.plot([0, max(expected)], [0, max(expected)], color='red', linestyle='--')\n",
    "    plt.xlabel('Expected -log10(P-value)')\n",
    "    plt.ylabel('Observed -log10(P-value)')\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f0239",
   "metadata": {},
   "source": [
    "Apply the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea2b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, beta_real = simulate_pheno(standardized_X, idx_caus, var_expl, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d266835c",
   "metadata": {},
   "source": [
    "##### LD Pruning for PCA & PCA\n",
    "\n",
    "plink --bfile data/final/chr22_step3_sampleqc \\\n",
    "  --indep-pairwise 200 50 0.2 \\\n",
    "  --out chr22_prune\n",
    "\n",
    "plink --bfile data/final/chr22_step3_sampleqc \\\n",
    "  --extract chr22_prune.prune.in \\\n",
    "  --pca 10 \\\n",
    "  --out chr22_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf143a1",
   "metadata": {},
   "source": [
    "##### 1) build phenotype table with IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60573f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno = fam[[\"fid\",\"iid\"]].copy()\n",
    "pheno.columns = [\"FID\",\"IID\"]\n",
    "pheno[\"y\"] = y.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b409cf9f",
   "metadata": {},
   "source": [
    "##### 2) load PCs and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = pd.read_csv(\"/Users/oykusuoglu/gobi/gobi_gwas/oyku/data/pca/chr22_pca.eigenvec\", sep=r\"\\s+\", header=None, engine=\"python\")\n",
    "pcs.columns = [\"FID\",\"IID\"] + [f\"PC{i}\" for i in range(1, pcs.shape[1]-1)]\n",
    "\n",
    "df = pheno.merge(pcs, on=[\"FID\",\"IID\"], how=\"inner\", validate=\"one_to_one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8af90",
   "metadata": {},
   "source": [
    "##### 3) build F (intercept + PCs) for LMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e49cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "F = np.column_stack([np.ones((df.shape[0], 1)),\n",
    "                     df[[f\"PC{i}\" for i in range(1, k+1)]].to_numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d73706d",
   "metadata": {},
   "source": [
    "### Phase 2: GWAS / Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b94b01",
   "metadata": {},
   "source": [
    "Now we have:\n",
    "\n",
    "- our genotype matrix **`standardized_X`** which was first standardized (before that it was **`X_real`**) and then used to simulate the phenotype matrix\n",
    "- our simulated phenotype matrix **`y`** \n",
    "- our covariates (PCs) **`F`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6cce02",
   "metadata": {},
   "source": [
    "#### 1. Training/Validation Set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f94315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, F_train, F_test = train_test_split(\n",
    "    standardized_X, y, F,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a018ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, F_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca6bc1a",
   "metadata": {},
   "source": [
    "#### 2. GWAS\n",
    "\n",
    "GWAS is only peformed on the training set (training genotype matrix **`X_train`** and phenotype matrix **`y_train`**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3043dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmm = LMM(y_train, F_train)\n",
    "lmm.process(X_train)\n",
    "pv = lmm.getPv()\n",
    "beta = lmm.getBetaSNP()\n",
    "beta_ste = lmm.getBetaSNPste()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ed11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d53fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(pv).any(), np.isnan(pv).sum(), np.where(np.isnan(pv)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qq_plot(pv, 'QQ training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f785fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bim_kept['pos'].values\n",
    "plt.subplot(211)\n",
    "plt.title('Real effect size')\n",
    "plt.plot(x, beta_real, '.k')\n",
    "plt.ylabel('eff size')\n",
    "plt.subplot(212)\n",
    "plt.title('GWAS results')\n",
    "plt.plot(x, -np.log10(pv), '.k')\n",
    "plt.ylabel('-log$_{10}$ P')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49ca3e5",
   "metadata": {},
   "source": [
    "#### 3. Clumping using PLINK "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dedee4",
   "metadata": {},
   "source": [
    "First create a file with SNP ids and their corresponding p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2443f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_for_clump = pd.DataFrame({\n",
    "    \"SNP\": bim_kept[\"snp\"].astype(str),   \n",
    "    \"P\": np.asarray(pv, dtype=float)\n",
    "})\n",
    "gwas_for_clump.to_csv(\"gwas_for_clump.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ffc22",
   "metadata": {},
   "source": [
    "clump-p1: p-val threshold for index SNPs\n",
    "clump-p2: secondary p val threshold for SNPs\n",
    "clump-r2: LD threshold for joining a SNP to an index SNPs clump \n",
    "clump-kb: window radius around the index SNP \n",
    "\n",
    "can try playing around with the parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4c43d",
   "metadata": {},
   "source": [
    "plink \\\n",
    "  --bfile data/final/chr22_step3_sampleqc \\\n",
    "  --clump gwas_for_clump.txt \\\n",
    "  --clump-snp-field SNP \\\n",
    "  --clump-field P \\\n",
    "  --clump-p1 1e-4 \\\n",
    "  --clump-p2 1e-2 \\\n",
    "  --clump-r2 0.1 \\\n",
    "  --clump-kb 250 \\\n",
    "  --out data/chr22_clumped\n",
    "\n",
    "\n",
    "plink \\\n",
    "  --bfile data/final/chr22_step3_sampleqc \\\n",
    "  --clump gwas_for_clump.txt \\\n",
    "  --clump-snp-field SNP \\\n",
    "  --clump-field P \\\n",
    "  --clump-p1 0.1 \\\n",
    "  --clump-r2 0.1 \\\n",
    "  --clump-kb 500 \\\n",
    "  --out chr22_clumped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0806cb7",
   "metadata": {},
   "source": [
    "try these: --clump-p1 1 --clump-r2 0.1 --clump-kb 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fb1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snp IDs used in GWAS (aligned with pv)\n",
    "snps_gwas = set(bim[\"snp\"].astype(str))   \n",
    "\n",
    "# SNP IDs in PLINK reference\n",
    "bim_ref = pd.read_csv(\"data/qc/chr22_step3_sampleqc.bim\", sep=r\"\\s+\", header=None)\n",
    "bim_ref.columns = [\"chrom\",\"snp\",\"cm\",\"pos\",\"a1\",\"a2\"]\n",
    "snps_ref = set(bim_ref[\"snp\"].astype(str))\n",
    "\n",
    "missing = snps_gwas - snps_ref\n",
    "print(\"missing in plink ref:\", len(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "clumped = pd.read_csv(\"/Users/oykusuoglu/gobi/gobi_gwas/oyku/data/chr22_clumped.clumped\", sep=r\"\\s+\", engine=\"python\")\n",
    "lead_snps = clumped[\"SNP\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee13638",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lead_snps), lead_snps[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_to_col = pd.Series(index=bim_kept[\"snp\"].astype(str), data=range(bim_kept.shape[0]))\n",
    "cols = snp_to_col.reindex(lead_snps).dropna().astype(int).to_numpy()\n",
    "cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcfc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which causal variants made it into your lead SNPs?\n",
    "causal_snp_names = set(bim_kept.iloc[idx_caus][\"snp\"].astype(str))\n",
    "recovered = causal_snp_names.intersection(set(lead_snps))\n",
    "print(f\"Directly recovered: {len(recovered)}/10 causal variants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each causal variant NOT in lead_snps, find its LD with lead SNPs\n",
    "causal_not_recovered = causal_snp_names - set(lead_snps)\n",
    "\n",
    "# Check if any lead SNP is within the clumping window (250kb) of these causal variants\n",
    "for snp in causal_not_recovered:\n",
    "    causal_pos = bim_kept[bim_kept[\"snp\"] == snp][\"pos\"].values[0]\n",
    "    nearby_leads = [ls for ls in lead_snps \n",
    "                    if abs(bim_kept[bim_kept[\"snp\"] == ls][\"pos\"].values[0] - causal_pos) < 250000]\n",
    "    print(f\"{snp}: {len(nearby_leads)} lead SNPs within 250kb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clump_train = X_train[:, cols] \n",
    "X_clump_test = X_test[:, cols]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c2b34",
   "metadata": {},
   "source": [
    "### Phase 3: Model Development\n",
    "\n",
    "Train a regression model to predict the phenotype based on the selected SNPs and test it on the\n",
    "validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5913afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14b388",
   "metadata": {},
   "source": [
    "**`X_clump_train`**: training genotype matrix only including clumped SNPs\n",
    "\n",
    "**`X_clump_test`**: test genotype matrix only including clumped SNPs\n",
    "\n",
    "**`y_train`**: training phenotypes\n",
    "\n",
    "**`y_test`**: test phenotypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_clump_train, y_train)\n",
    "\n",
    "y_pred = regr.predict(X_clump_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26256673",
   "metadata": {},
   "source": [
    "R2: close to 1 ~ good performance\n",
    "\n",
    "MSE: close to 0 ~ good performance \n",
    "\n",
    "spearman r: \n",
    "- close to -1 ~ monotonic negative correlation\n",
    "- close to 1 ~ monotonic positive correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109bb0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "spearman = stats.spearmanr(y_test, y_pred)\n",
    "print(\"R2\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"spearman r: \", spearman.statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3479a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(y_pred, input, model):\n",
    "    plt.scatter(y_test, y_pred)\n",
    "\n",
    "    xx = np.linspace(y_pred.min(), y_pred.max(), 200)\n",
    "\n",
    "    plt.plot(xx, xx, linewidth=2, linestyle=\"--\", label=\"ideal: y=x\", color=\"orange\")\n",
    "    plt.title(f\"{model} with {input}\")\n",
    "    plt.xlabel(\"Phenotype true\")\n",
    "    plt.ylabel(\"Phenotype predicted\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aff620",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(y_pred, input=\"Causal Variants: 10 heritability: 0.6\", model=\"Linear Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f71422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w_pcs = np.hstack([X_clump_train, F_train[:, 1:]])  # F[:, 1:] excludes intercept\n",
    "X_test_w_pcs = np.hstack([X_clump_test, F_test[:, 1:]])\n",
    "\n",
    "regr.fit(X_train_w_pcs, y_train)\n",
    "y_pred_w_pcs = regr.predict(X_test_w_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ced91",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_w_pcs = stats.spearmanr(y_test, y_pred_w_pcs)\n",
    "print(\"R2\", r2_score(y_test, y_pred_w_pcs))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_w_pcs))\n",
    "print(\"spearman r: \", spearman_w_pcs.statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffe0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(y_pred_w_pcs, input=\"Causal Variants: 10 heritability: 0.6 including PCs as Covariates\", model=\"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c16df1",
   "metadata": {},
   "source": [
    "#### Predict with Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71650b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(8, 16),\n",
    "    activation=\"relu\",\n",
    "    max_iter=1000,\n",
    "    alpha=0.001,\n",
    "    beta_1=0.5,\n",
    "    random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b43dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(X_train_w_pcs, y_train)\n",
    "y_pred_w_mlp = mlp.predict(X_test_w_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeeaf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_w_mlp = stats.spearmanr(y_test, y_pred_w_mlp)\n",
    "print(\"R2\", r2_score(y_test, y_pred_w_mlp))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_w_mlp))\n",
    "print(\"spearman r: \", spearman_w_mlp.statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a661fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(y_pred_w_mlp, input=\"Causal Variants: 10 heritability: 0.6 including PCs as Covariates\", model=\"MLP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gobi_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
